{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data = pd.read_csv(\"data/data_health/trace_activities.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.drop(columns=[\"index\", \"EVENTID\"])\n",
    "data[\"start\"] = pd.to_datetime(data[\"start\"])\n",
    "data[\"end\"] = pd.to_datetime(data[\"end\"])\n",
    "\n",
    "attributes = [\n",
    "    attr\n",
    "    for attr in data.select_dtypes(include=[\"object\", \"bool\", \"number\"]).columns\n",
    "    if attr not in [\"traceId\", \"activity\", \"start\", \"end\"]\n",
    "]\n",
    "\n",
    "\n",
    "def is_trace_level(attribute):\n",
    "    return data.groupby(\"traceId\")[attribute].nunique().max() == 1\n",
    "\n",
    "\n",
    "selected_attributes = [attr for attr in attributes if is_trace_level(attr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"activity_duration\"] = (data[\"end\"] - data[\"start\"]).dt.total_seconds().astype(int)\n",
    "\n",
    "data[\"activity_durations\"] = data.groupby(\"traceId\")[\"activity_duration\"].transform(\n",
    "    lambda x: [x.tolist()] * len(x)\n",
    ")\n",
    "\n",
    "data[\"transition_duration\"] = (\n",
    "    (data.groupby(\"traceId\")[\"start\"].shift(-1) - data[\"end\"])\n",
    "    .dt.total_seconds()\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "\n",
    "data[\"transition_durations\"] = data.groupby(\"traceId\")[\"transition_duration\"].transform(\n",
    "    lambda x: [x.tolist()] * len(x)\n",
    ")\n",
    "\n",
    "trace_total_duration = (\n",
    "    data.groupby(\"traceId\")\n",
    "    .apply(lambda x: (x[\"end\"].max() - x[\"start\"].min()).total_seconds())\n",
    "    .reset_index(name=\"trace_total_duration\")\n",
    ")\n",
    "\n",
    "\n",
    "data = pd.merge(data, trace_total_duration, on=\"traceId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = data[\"activity\"].unique().tolist()\n",
    "activity_to_index = {activity: i for i, activity in enumerate(activities)}\n",
    "data[\"activity\"] = data[\"activity\"].map(activity_to_index)\n",
    "\n",
    "\n",
    "def decode_activities(indices, index_to_activity):\n",
    "    return [index_to_activity[index] for index in indices]\n",
    "\n",
    "\n",
    "data[\"trace_activity_list\"] = data.groupby(\"traceId\")[\"activity\"].transform(\n",
    "    lambda x: [x.tolist()] * len(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset=\"traceId\")[\n",
    "    [\n",
    "        \"traceId\",\n",
    "        \"trace_activity_list\",\n",
    "        \"activity_durations\",\n",
    "        \"transition_durations\",\n",
    "        \"trace_total_duration\",\n",
    "    ]\n",
    "    + selected_attributes\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All durations are consistent.\n"
     ]
    }
   ],
   "source": [
    "def assert_duration_consistency(row):\n",
    "    activity_sum = sum(row[\"activity_durations\"])\n",
    "    transition_sum = sum(row[\"transition_durations\"])\n",
    "    total_duration = row[\"trace_total_duration\"]\n",
    "    assert (\n",
    "        activity_sum + transition_sum == total_duration\n",
    "    ), f\"Inconsistency found in trace {row['traceId']}: {activity_sum} (activities) + {transition_sum} (transitions) != {total_duration} (total)\"\n",
    "\n",
    "\n",
    "data.apply(assert_duration_consistency, axis=1)\n",
    "print(\"All durations are consistent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['traceId', 'trace_activity_list', 'activity_durations',\n",
       "       'transition_durations', 'trace_total_duration', 'CORDERID', 'NOTKZ',\n",
       "       'EINRI_Badajoz', 'EINRI_Castellon', 'EINRI_Teruel', 'ENDDT_Abierto',\n",
       "       'ENDDT_Cerrado', 'LSSTAE_DI', 'LSSTAE_ER', 'LSSTAE_QU', 'LSSTAE_UA',\n",
       "       'MANDT_Doctor_1', 'MANDT_Doctor_10', 'MANDT_Doctor_11',\n",
       "       'MANDT_Doctor_2', 'MANDT_Doctor_3', 'MANDT_Doctor_4', 'MANDT_Doctor_5',\n",
       "       'MANDT_Doctor_6', 'MANDT_Doctor_7', 'MANDT_Doctor_8', 'MANDT_Doctor_9',\n",
       "       'STATU_50.0', 'STATU_60.0', 'STATU_70.0', 'STATU_True', 'STORN_Empty',\n",
       "       'STORN_x'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in data[selected_attributes].select_dtypes(include=[\"bool\"]).columns:\n",
    "    data[col] = data[col].astype(int)\n",
    "\n",
    "data = pd.get_dummies(\n",
    "    data,\n",
    "    columns=data[selected_attributes]\n",
    "    .select_dtypes(include=[\"object\", \"category\"])\n",
    "    .columns,\n",
    ")\n",
    "\n",
    "transformed_attributes = data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['EINRI', 'ENDDT', 'LSSTAE', 'MANDT', 'STATU', 'STORN'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m##TODO Update attributes here!!\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[1;32m----> 5\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mselected_attributes\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrace_total_duration\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\mycke\\Documents\\Inverbis\\new_model\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\mycke\\Documents\\Inverbis\\new_model\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mycke\\Documents\\Inverbis\\new_model\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['EINRI', 'ENDDT', 'LSSTAE', 'MANDT', 'STATU', 'STORN'] not in index\""
     ]
    }
   ],
   "source": [
    "##TODO Update attributes here!!\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X = data[selected_attributes]\n",
    "y = data[\"trace_total_duration\"]\n",
    "\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "feature_importances = pd.DataFrame(\n",
    "    {\"feature\": X.columns, \"importance\": model.feature_importances_}\n",
    ").sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "feature_importances[\"cumulative_importance\"] = feature_importances[\n",
    "    \"importance\"\n",
    "].cumsum()\n",
    "\n",
    "important_attributes = feature_importances[\n",
    "    feature_importances[\"cumulative_importance\"] <= 0.95\n",
    "][\"feature\"].tolist()\n",
    "\n",
    "attributes_to_exclude = list(set(selected_attributes) - set(important_attributes))\n",
    "data = data.drop(columns=attributes_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def truncate_sequence_random(seq):\n",
    "    if len(seq) > 1:\n",
    "        trunc_point = np.random.randint(1, len(seq))\n",
    "        truncated = seq[:trunc_point]\n",
    "        remaining = seq[trunc_point:]\n",
    "    else:\n",
    "        truncated = seq\n",
    "        remaining = []\n",
    "        trunc_point = len(seq)\n",
    "    return truncated, remaining, trunc_point\n",
    "\n",
    "\n",
    "def truncate_list(lst, trunc_points, offset=0):\n",
    "    truncated = [\n",
    "        item[: truncation_point - offset]\n",
    "        for item, truncation_point in zip(lst, trunc_points)\n",
    "    ]\n",
    "    remaining = [\n",
    "        item[truncation_point - offset :]\n",
    "        for item, truncation_point in zip(lst, trunc_points)\n",
    "    ]\n",
    "    return truncated, remaining\n",
    "\n",
    "\n",
    "data[[\"truncated_activity_list\", \"remaining_activity_list\", \"trunc_point\"]] = (\n",
    "    data[\"trace_activity_list\"].apply(truncate_sequence_random).apply(pd.Series)\n",
    ")\n",
    "\n",
    "data[\"truncated_durations\"], data[\"remaining_durations\"] = truncate_list(\n",
    "    data[\"activity_durations\"], data[\"trunc_point\"]\n",
    ")\n",
    "data[\"truncated_transitions\"], data[\"remaining_transitions\"] = truncate_list(\n",
    "    data[\"transition_durations\"], data[\"trunc_point\"], offset=1\n",
    ")\n",
    "\n",
    "data[\"truncated_total_duration\"] = data[\"truncated_durations\"].apply(sum) + data[\n",
    "    \"truncated_transitions\"\n",
    "].apply(sum)\n",
    "data[\"remaining_total_duration\"] = data[\"remaining_durations\"].apply(sum) + data[\n",
    "    \"remaining_transitions\"\n",
    "].apply(sum)\n",
    "\n",
    "assert all(\n",
    "    data[\"truncated_total_duration\"] + data[\"remaining_total_duration\"]\n",
    "    == data[\"trace_total_duration\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is probably going to be removed, I leave it here just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the lengths of each trace_activity_list\n",
    "# data[\"sequence_length\"] = data[\"trace_activity_list\"].apply(len)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.hist(\n",
    "#     data[\"sequence_length\"],\n",
    "#     bins=range(1, data[\"sequence_length\"].max() + 1),\n",
    "#     color=\"blue\",\n",
    "#     alpha=0.5,\n",
    "#     label=\"Sequence Lengths\",\n",
    "# )\n",
    "# ax.hist(\n",
    "#     data[\"trunc_point\"],\n",
    "#     bins=range(1, data[\"sequence_length\"].max() + 1),\n",
    "#     color=\"red\",\n",
    "#     alpha=0.5,\n",
    "#     label=\"Truncation Points\",\n",
    "# )\n",
    "# ax.set_title(\"Comparison of Sequence Lengths and Truncation Points\")\n",
    "# ax.set_xlabel(\"Point in Sequence\")\n",
    "# ax.set_ylabel(\"Frequency\")\n",
    "# ax.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_sequence_length = max(data[\"trace_activity_list\"].apply(len))\n",
    "\n",
    "data[\"truncated_activity_list\"] = pad_sequences(\n",
    "    data[\"truncated_activity_list\"], maxlen=max_sequence_length, padding=\"post\"\n",
    ").tolist()\n",
    "\n",
    "data[\"remaining_activity_list\"] = pad_sequences(\n",
    "    data[\"remaining_activity_list\"], maxlen=max_sequence_length, padding=\"post\"\n",
    ").tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
