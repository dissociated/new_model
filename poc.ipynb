{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "data = pd.read_csv(\"data/data_health/trace_activities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.drop(columns=[\"index\", \"EVENTID\"])\n",
    "data[\"start\"] = pd.to_datetime(data[\"start\"])\n",
    "data[\"end\"] = pd.to_datetime(data[\"end\"])\n",
    "\n",
    "n_unique_activities = len(data[\"activity\"].unique()) + 1\n",
    "\n",
    "attributes = [\n",
    "    attr\n",
    "    for attr in data.select_dtypes(include=[\"object\", \"bool\", \"number\"]).columns\n",
    "    if attr not in [\"traceId\", \"activity\", \"start\", \"end\"]\n",
    "]\n",
    "\n",
    "\n",
    "def is_trace_level(attribute):\n",
    "    return data.groupby(\"traceId\")[attribute].nunique().max() == 1\n",
    "\n",
    "\n",
    "selected_attributes = [attr for attr in attributes if is_trace_level(attr)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We get the durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"activity_duration\"] = (data[\"end\"] - data[\"start\"]).dt.total_seconds().astype(int)\n",
    "\n",
    "data[\"activity_durations\"] = data.groupby(\"traceId\")[\"activity_duration\"].transform(\n",
    "    lambda x: [x.tolist()] * len(x)\n",
    ")\n",
    "\n",
    "data[\"transition_duration\"] = (\n",
    "    (data.groupby(\"traceId\")[\"start\"].shift(-1) - data[\"end\"])\n",
    "    .dt.total_seconds()\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")\n",
    "\n",
    "\n",
    "data[\"transition_durations\"] = data.groupby(\"traceId\")[\"transition_duration\"].transform(\n",
    "    lambda x: [x.tolist()] * len(x)\n",
    ")\n",
    "\n",
    "trace_total_duration = (\n",
    "    data.groupby(\"traceId\")\n",
    "    .apply(lambda x: (x[\"end\"].max() - x[\"start\"].min()).total_seconds())\n",
    "    .reset_index(name=\"trace_total_duration\")\n",
    ")\n",
    "\n",
    "\n",
    "data = pd.merge(data, trace_total_duration, on=\"traceId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the activities and get the activities list / Take only the first row and assert durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All durations are consistent.\n"
     ]
    }
   ],
   "source": [
    "activities = data[\"activity\"].unique().tolist()\n",
    "activity_to_index = {activity: i for i, activity in enumerate(activities)}\n",
    "data[\"activity\"] = data[\"activity\"].map(activity_to_index)\n",
    "\n",
    "\n",
    "def decode_activities(indices, index_to_activity):\n",
    "    return [index_to_activity[index] for index in indices]\n",
    "\n",
    "\n",
    "data[\"trace_activity_list\"] = data.groupby(\"traceId\")[\"activity\"].transform(\n",
    "    lambda x: [x.tolist()] * len(x)\n",
    ")\n",
    "\n",
    "data = data.drop_duplicates(subset=\"traceId\")[\n",
    "    [\n",
    "        \"traceId\",\n",
    "        \"trace_activity_list\",\n",
    "        \"activity_durations\",\n",
    "        \"transition_durations\",\n",
    "        \"trace_total_duration\",\n",
    "    ]\n",
    "    + selected_attributes\n",
    "]\n",
    "\n",
    "\n",
    "def assert_duration_consistency(row):\n",
    "    activity_sum = sum(row[\"activity_durations\"])\n",
    "    transition_sum = sum(row[\"transition_durations\"])\n",
    "    total_duration = row[\"trace_total_duration\"]\n",
    "    assert (\n",
    "        activity_sum + transition_sum == total_duration\n",
    "    ), f\"Inconsistency found in trace {row['traceId']}: {activity_sum} (activities) + {transition_sum} (transitions) != {total_duration} (total)\"\n",
    "\n",
    "\n",
    "data.apply(assert_duration_consistency, axis=1)\n",
    "print(\"All durations are consistent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_columns = data[selected_attributes].select_dtypes(include=[\"bool\"]).columns\n",
    "for col in boolean_columns:\n",
    "    data[col] = data[col].astype(int)\n",
    "\n",
    "initial_columns = data.columns.tolist()\n",
    "\n",
    "data = pd.get_dummies(\n",
    "    data,\n",
    "    columns=data[selected_attributes]\n",
    "    .select_dtypes(include=[\"object\", \"category\"])\n",
    "    .columns,\n",
    ")\n",
    "\n",
    "new_dummy_columns = list(set(data.columns) - set(initial_columns))\n",
    "transformed_columns = list(boolean_columns) + new_dummy_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncate sequences and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def truncate_sequence_random(seq):\n",
    "    if len(seq) > 1:\n",
    "        trunc_point = np.random.randint(1, len(seq))\n",
    "        truncated = seq[:trunc_point]\n",
    "        remaining = seq[trunc_point:]\n",
    "    else:\n",
    "        truncated = seq\n",
    "        remaining = []\n",
    "        trunc_point = len(seq)\n",
    "    return truncated, remaining, trunc_point\n",
    "\n",
    "\n",
    "def truncate_list(lst, trunc_points, offset=0):\n",
    "    truncated = [\n",
    "        item[: truncation_point - offset]\n",
    "        for item, truncation_point in zip(lst, trunc_points)\n",
    "    ]\n",
    "    remaining = [\n",
    "        item[truncation_point - offset :]\n",
    "        for item, truncation_point in zip(lst, trunc_points)\n",
    "    ]\n",
    "    return truncated, remaining\n",
    "\n",
    "\n",
    "data[[\"truncated_activity_list\", \"remaining_activity_list\", \"trunc_point\"]] = (\n",
    "    data[\"trace_activity_list\"].apply(truncate_sequence_random).apply(pd.Series)\n",
    ")\n",
    "\n",
    "data[\"truncated_durations\"], data[\"remaining_durations\"] = truncate_list(\n",
    "    data[\"activity_durations\"], data[\"trunc_point\"]\n",
    ")\n",
    "data[\"truncated_transitions\"], data[\"remaining_transitions\"] = truncate_list(\n",
    "    data[\"transition_durations\"], data[\"trunc_point\"], offset=1\n",
    ")\n",
    "\n",
    "data[\"truncated_total_duration\"] = data[\"truncated_durations\"].apply(sum) + data[\n",
    "    \"truncated_transitions\"\n",
    "].apply(sum)\n",
    "data[\"remaining_total_duration\"] = data[\"remaining_durations\"].apply(sum) + data[\n",
    "    \"remaining_transitions\"\n",
    "].apply(sum)\n",
    "\n",
    "assert all(\n",
    "    data[\"truncated_total_duration\"] + data[\"remaining_total_duration\"]\n",
    "    == data[\"trace_total_duration\"]\n",
    ")\n",
    "\n",
    "max_sequence_length = max(data[\"trace_activity_list\"].apply(len))\n",
    "\n",
    "data[\"truncated_activity_list\"] = pad_sequences(\n",
    "    data[\"truncated_activity_list\"], maxlen=max_sequence_length, padding=\"post\"\n",
    ").tolist()\n",
    "\n",
    "data[\"remaining_activity_list\"] = pad_sequences(\n",
    "    data[\"remaining_activity_list\"], maxlen=max_sequence_length, padding=\"post\"\n",
    ").tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Splitting, reshaping and one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = train_df.sort_values(by=\"traceId\")\n",
    "test_df = test_df.sort_values(by=\"traceId\")\n",
    "\n",
    "X_train_features = train_df[transformed_columns].values\n",
    "X_test_features = test_df[transformed_columns].values\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "X_train = np.array(train_df[\"truncated_activity_list\"].tolist())\n",
    "Y_train = np.array(train_df[\"remaining_activity_list\"].tolist())\n",
    "\n",
    "X_test = np.array(test_df[\"truncated_activity_list\"].tolist())\n",
    "Y_test = np.array(test_df[\"remaining_activity_list\"].tolist())\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "Y_train = Y_train.reshape(Y_train.shape[0], Y_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "Y_test = Y_test.reshape(Y_test.shape[0], Y_test.shape[1], 1)\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_train_features = X_train_features.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "X_test_features = X_test_features.astype(np.float32)\n",
    "\n",
    "Y_train_onehot = to_categorical(Y_train.squeeze(), num_classes=n_unique_activities)\n",
    "Y_test_onehot = to_categorical(Y_test.squeeze(), num_classes=n_unique_activities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "def mask_acc(y_true, y_pred):\n",
    "    mask = K.cast(K.max(y_true, axis=-1), K.floatx())\n",
    "\n",
    "    y_true_labels = K.cast(K.argmax(y_true, axis=-1), K.floatx())\n",
    "    y_pred_labels = K.cast(K.argmax(y_pred, axis=-1), K.floatx())\n",
    "\n",
    "    non_zero_mask = K.cast(K.greater(y_true_labels, 0), K.floatx())\n",
    "\n",
    "    is_correct = (\n",
    "        K.cast(K.equal(y_true_labels, y_pred_labels), K.floatx()) * mask * non_zero_mask\n",
    "    )\n",
    "    total_correct = K.sum(is_correct)\n",
    "    total_values = K.sum(mask * non_zero_mask)\n",
    "\n",
    "    return total_correct / total_values\n",
    "\n",
    "\n",
    "def seq_acc(y_true, y_pred):\n",
    "    y_pred_labels = K.argmax(y_pred, axis=-1)\n",
    "    y_true_labels = K.argmax(y_true, axis=-1)\n",
    "\n",
    "    correct_preds = K.all(K.equal(y_true_labels, y_pred_labels), axis=-1)\n",
    "\n",
    "    accuracy = K.mean(correct_preds)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    Embedding,\n",
    "    LSTM,\n",
    "    Dense,\n",
    "    Concatenate,\n",
    "    Bidirectional,\n",
    "    RepeatVector,\n",
    "    Dropout,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "sequence_input = Input(shape=(X_train.shape[1],))\n",
    "embedded_sequences = Embedding(input_dim=n_unique_activities, output_dim=64)(\n",
    "    sequence_input\n",
    ")\n",
    "lstm_out = Bidirectional(LSTM(64, return_sequences=True))(embedded_sequences)\n",
    "lstm_out = Dropout(0.15)(lstm_out)  # Add dropout after LSTM\n",
    "\n",
    "feature_input = Input(shape=(X_train_features.shape[1],))\n",
    "dense_feature = Dense(64, activation=\"relu\")(feature_input)\n",
    "dense_feature = Dropout(0.15)(dense_feature)  # Add dropout after first Dense layer\n",
    "dense_feature = Dense(64, activation=\"relu\")(dense_feature)\n",
    "repeated_feature = RepeatVector(X_train.shape[1])(dense_feature)\n",
    "\n",
    "concatenated = Concatenate(axis=-1)([lstm_out, repeated_feature])\n",
    "combined_dense = Dense(64, activation=\"relu\")(concatenated)\n",
    "\n",
    "output = Dense(n_unique_activities, activation=\"softmax\")(combined_dense)\n",
    "\n",
    "model = Model(inputs=[sequence_input, feature_input], outputs=output)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=Adam(learning_rate=0.005),\n",
    "    metrics=[mask_acc, seq_acc],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m287/287\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m72/72\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "predicted_sequences1 = model.predict([X_train, X_train_features])\n",
    "predicted_sequences2 = model.predict([X_test, X_test_features])\n",
    "\n",
    "predicted_activity_indices1 = [np.argmax(seq, axis=-1) for seq in predicted_sequences1]\n",
    "predicted_activity_indices2 = [np.argmax(seq, axis=-1) for seq in predicted_sequences2]\n",
    "\n",
    "train_df['predicted_sequence'] = predicted_activity_indices1\n",
    "test_df['predicted_sequence'] = predicted_activity_indices2\n",
    "\n",
    "combined_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
    "combined_df = combined_df[['traceId', 'predicted_sequence']].copy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
